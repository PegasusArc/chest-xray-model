# -*- coding: utf-8 -*-
"""CHEST X-RAY DISEASE CLASSIFICATION SYSTEM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mnK6LpD1IyF_TQYjFYd41FKISHG30roD
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import json

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision
import torchvision.transforms as transforms
import torchvision.models as models
from PIL import Image

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    roc_auc_score,
    roc_curve,
    auc,
    f1_score,
    precision_score,
    recall_score,
    confusion_matrix,
    accuracy_score
)

torch.manual_seed(42)
np.random.seed(42)

print(f"PyTorch Version: {torch.__version__}")
print(f"Torchvision Version: {torchvision.__version__}")
print(f"CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

from google.colab import files

print("üìÅ Please upload your kaggle.json file:")
uploaded = files.upload()

# Setup Kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

print("\nüì• Downloading NIH Chest X-ray Dataset (this may take 5-10 minutes)...")
!kaggle datasets download -d nih-chest-xrays/sample
!unzip -q sample.zip -d chest_xray_data

print("‚úÖ Dataset downloaded!")

# Paths
DATA_DIR = "chest_xray_data/sample/images"
LABELS_FILE = "chest_xray_data/sample_labels.csv"

df = pd.read_csv(LABELS_FILE)

print(f"\nüìä Dataset Overview:")
print(f"Total images: {len(df)}")
print(f"Total patients: {df['Patient ID'].nunique()}")

# Disease labels
DISEASE_LABELS = [
    'Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax',
    'Edema', 'Emphysema', 'Fibrosis', 'Effusion', 'Pneumonia',
    'Pleural_thickening', 'Cardiomegaly', 'Nodule', 'Mass', 'Hernia'
]

# Create binary columns for each disease
for disease in DISEASE_LABELS:
    df[disease] = df['Finding Labels'].apply(lambda x: 1 if disease in x else 0)

# Count diseases
print(f"\nüíä Disease Distribution:")
disease_counts = df[DISEASE_LABELS].sum().sort_values(ascending=False)
print(disease_counts)

# Visualize disease distribution
plt.figure(figsize=(12, 6))
disease_counts.plot(kind='bar', color='steelblue')
plt.title('Disease Distribution in Dataset', fontsize=14, fontweight='bold')
plt.xlabel('Disease', fontsize=12)
plt.ylabel('Number of Cases', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('disease_distribution.png', dpi=100)
plt.show()

class FocalLoss(nn.Module):
    """
    Focal Loss focuses more on hard-to-classify examples (sick people!)
    This helps the model pay more attention to minority class (diseases)
    """
    def __init__(self, alpha=0.25, gamma=2.0, pos_weight=None):
        super(FocalLoss, self).__init__()
        self.alpha = alpha
        self.gamma = gamma
        self.pos_weight = pos_weight

    def forward(self, inputs, targets):
        # BCE loss
        bce_loss = nn.functional.binary_cross_entropy_with_logits(
            inputs, targets, reduction='none', pos_weight=self.pos_weight
        )

        # Get probabilities
        probs = torch.sigmoid(inputs)

        # Focal term: (1 - p)^gamma for positive class, p^gamma for negative class
        p_t = probs * targets + (1 - probs) * (1 - targets)
        focal_term = (1 - p_t) ** self.gamma

        # Alpha weighting
        alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)

        # Focal loss
        loss = alpha_t * focal_term * bce_loss

        return loss.mean()

class ChestXrayDataset(Dataset):
    """Custom Dataset for Chest X-ray images"""

    def __init__(self, dataframe, image_dir, labels, transform=None):
        self.dataframe = dataframe.reset_index(drop=True)
        self.image_dir = image_dir
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        img_name = self.dataframe.iloc[idx]['Image Index']
        img_path = os.path.join(self.image_dir, img_name)

        try:
            image = Image.open(img_path).convert('RGB')
        except:
            image = Image.new('RGB', (224, 224), color='black')
            print(f"Warning: Failed to load {img_name}")

        label_vector = self.dataframe.iloc[idx][self.labels].values.astype(np.float32)

        if self.transform:
            image = self.transform(image)

        return image, torch.tensor(label_vector)

# Data transforms with stronger augmentation
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(15),  # Increased rotation
    transforms.ColorJitter(brightness=0.3, contrast=0.3),  # Increased
    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # Added translation
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

def split_dataset(df, test_size=0.15, val_size=0.15, random_state=42):
    """Split dataset by patient ID to prevent data leakage"""
    patients = df['Patient ID'].unique()

    train_patients, test_patients = train_test_split(
        patients, test_size=test_size, random_state=random_state
    )
    train_patients, val_patients = train_test_split(
        train_patients, test_size=val_size/(1-test_size), random_state=random_state
    )

    train_df = df[df['Patient ID'].isin(train_patients)]
    val_df = df[df['Patient ID'].isin(val_patients)]
    test_df = df[df['Patient ID'].isin(test_patients)]

    print(f"\nüìä Data Split:")
    print(f"Train: {len(train_df)} images ({len(train_patients)} patients)")
    print(f"Val:   {len(val_df)} images ({len(val_patients)} patients)")
    print(f"Test:  {len(test_df)} images ({len(test_patients)} patients)")

    return train_df, val_df, test_df

# Split data
train_df, val_df, test_df = split_dataset(df)

# Create datasets
train_dataset = ChestXrayDataset(train_df, DATA_DIR, DISEASE_LABELS, train_transform)
val_dataset = ChestXrayDataset(val_df, DATA_DIR, DISEASE_LABELS, val_transform)
test_dataset = ChestXrayDataset(test_df, DATA_DIR, DISEASE_LABELS, val_transform)

# Create dataloaders
BATCH_SIZE = 32

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,
                         num_workers=2, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,
                       num_workers=2, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,
                        num_workers=2, pin_memory=True)

print(f"\n‚úÖ Dataloaders created:")
print(f"Train batches: {len(train_loader)}")
print(f"Val batches: {len(val_loader)}")
print(f"Test batches: {len(test_loader)}")

class ChestXrayModel(nn.Module):
    """DenseNet-121 for multi-label chest X-ray classification"""

    def __init__(self, num_classes=14, pretrained=True, dropout=0.3):
        super(ChestXrayModel, self).__init__()

        self.densenet = models.densenet121(pretrained=pretrained)
        num_features = self.densenet.classifier.in_features

        # Improved classifier with batch norm
        self.densenet.classifier = nn.Sequential(
            nn.BatchNorm1d(num_features),
            nn.Dropout(p=dropout),
            nn.Linear(num_features, 512),
            nn.ReLU(),
            nn.BatchNorm1d(512),
            nn.Dropout(p=dropout),
            nn.Linear(512, num_classes)
        )

    def forward(self, x):
        return self.densenet(x)

# Create model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = ChestXrayModel(num_classes=14, pretrained=True, dropout=0.3)
model = model.to(device)

total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nüß† Model Architecture:")
print(f"Total parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")
print(f"Device: {device}")

def calculate_class_weights(train_df, labels, boost_factor=3.0):
    """
    Calculate pos_weight with boost factor to prioritize sick detection
    boost_factor > 1 means we care MORE about finding sick people
    """
    pos_counts = train_df[labels].sum().values
    neg_counts = len(train_df) - pos_counts

    # Apply boost factor to make model more sensitive to diseases
    pos_weights = (neg_counts / (pos_counts + 1)) * boost_factor

    print(f"\n‚öñÔ∏è Class Weights (BOOSTED by {boost_factor}x for better sick detection):")
    for label, weight in zip(labels, pos_weights):
        print(f"{label:20s}: {weight:.2f}")

    return torch.tensor(pos_weights, dtype=torch.float32)

# Get boosted class weights
class_weights = calculate_class_weights(train_df, DISEASE_LABELS, boost_factor=3.0).to(device)

# Use Focal Loss instead of BCE
criterion = FocalLoss(alpha=0.25, gamma=2.0, pos_weight=class_weights)

# Optimizer with weight decay
optimizer = optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)

# Learning rate scheduler
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='max',
    factor=0.5,
    patience=2,
    min_lr=1e-7
)

print(f"\n‚úÖ Training Setup (OPTIMIZED FOR SICK DETECTION):")
print(f"Loss: Focal Loss (alpha=0.25, gamma=2.0) with 3x boosted weights")
print(f"Optimizer: AdamW (lr=0.0001, weight_decay=1e-4)")
print(f"Scheduler: ReduceLROnPlateau")

def train_one_epoch(model, loader, criterion, optimizer, device):
    """Train for one epoch"""
    model.train()
    running_loss = 0.0

    progress_bar = tqdm(loader, desc="Training")

    for images, labels in progress_bar:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)

        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

    return running_loss / len(loader)

def find_optimal_threshold(labels, predictions):
    """
    Find threshold that maximizes F1 score
    Lower threshold = more sensitive = better at finding sick people
    """
    thresholds = np.arange(0.1, 0.9, 0.05)
    best_threshold = 0.5
    best_f1 = 0.0

    for thresh in thresholds:
        pred_binary = (predictions > thresh).astype(int)
        f1 = f1_score(labels, pred_binary, average='macro', zero_division=0)
        if f1 > best_f1:
            best_f1 = f1
            best_threshold = thresh

    return best_threshold

def validate(model, loader, criterion, device, threshold=0.5):
    """Validate model with COMPREHENSIVE metrics for sick detection"""
    model.eval()
    running_loss = 0.0
    all_labels = []
    all_predictions = []

    with torch.no_grad():
        for images, labels in tqdm(loader, desc="Validating"):
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item()

            predictions = torch.sigmoid(outputs).cpu().numpy()
            all_predictions.append(predictions)
            all_labels.append(labels.cpu().numpy())

    all_predictions = np.vstack(all_predictions)
    all_labels = np.vstack(all_labels)

    avg_loss = running_loss / len(loader)

    # AUC-ROC per disease
    auc_scores = []
    for i in range(len(DISEASE_LABELS)):
        if len(np.unique(all_labels[:, i])) > 1:
            auc = roc_auc_score(all_labels[:, i], all_predictions[:, i])
            auc_scores.append(auc)
        else:
            auc_scores.append(0.0)

    mean_auc = np.mean([s for s in auc_scores if s > 0])

    # Binary predictions
    predictions_binary = (all_predictions > threshold).astype(int)

    # SICK DETECTION METRICS
    # Sensitivity (Recall) - Most important for finding sick people!
    sensitivity = recall_score(all_labels, predictions_binary, average='macro', zero_division=0)

    # Specificity - ability to identify healthy correctly
    tn_fp = np.sum(all_labels == 0, axis=0)
    tn = np.sum((all_labels == 0) & (predictions_binary == 0), axis=0)
    specificity_per_disease = tn / (tn_fp + 1e-10)
    specificity = np.mean(specificity_per_disease)

    # Standard metrics
    accuracy = accuracy_score(all_labels, predictions_binary)
    f1 = f1_score(all_labels, predictions_binary, average='macro', zero_division=0)
    precision = precision_score(all_labels, predictions_binary, average='macro', zero_division=0)

    return {
        'loss': avg_loss,
        'auc': mean_auc,
        'accuracy': accuracy,
        'f1': f1,
        'precision': precision,
        'recall': sensitivity,
        'sensitivity': sensitivity,  # Same as recall
        'specificity': specificity,
        'auc_per_disease': auc_scores,
        'predictions': all_predictions,
        'labels': all_labels
    }

NUM_EPOCHS = 27
EARLY_STOP_PATIENCE = 5

print(f"\nüöÄ Starting Training for {NUM_EPOCHS} epochs")
print("=" * 70)

history = {
    'train_loss': [],
    'val_loss': [],
    'val_auc': [],
    'val_accuracy': [],
    'val_f1': [],
    'val_precision': [],
    'val_recall': [],
    'val_sensitivity': [],
    'val_specificity': [],
    'learning_rate': []
}

best_auc = 0.0
patience_counter = 0

for epoch in range(NUM_EPOCHS):
    print(f"\nüìÖ Epoch {epoch + 1}/{NUM_EPOCHS}")
    print("-" * 70)

    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)
    val_metrics = validate(model, val_loader, criterion, device, threshold=0.4)  # Lower threshold!

    scheduler.step(val_metrics['auc'])
    current_lr = optimizer.param_groups[0]['lr']

    # Save history
    history['train_loss'].append(train_loss)
    history['val_loss'].append(val_metrics['loss'])
    history['val_auc'].append(val_metrics['auc'])
    history['val_accuracy'].append(val_metrics['accuracy'])
    history['val_f1'].append(val_metrics['f1'])
    history['val_precision'].append(val_metrics['precision'])
    history['val_recall'].append(val_metrics['recall'])
    history['val_sensitivity'].append(val_metrics['sensitivity'])
    history['val_specificity'].append(val_metrics['specificity'])
    history['learning_rate'].append(current_lr)

    print(f"\nüìä Results:")
    print(f"   Train Loss:     {train_loss:.4f}")
    print(f"   Val Loss:       {val_metrics['loss']:.4f}")
    print(f"   Val AUC:        {val_metrics['auc']:.4f} ‚≠ê")
    print(f"   Val Accuracy:   {val_metrics['accuracy']:.4f}")
    print(f"   Val F1:         {val_metrics['f1']:.4f}")
    print(f"   Val Precision:  {val_metrics['precision']:.4f}")
    print(f"   Val SENSITIVITY: {val_metrics['sensitivity']:.4f} üéØ (Finding Sick!)")
    print(f"   Val Specificity: {val_metrics['specificity']:.4f}")
    print(f"   Learning Rate:  {current_lr:.2e}")

    if val_metrics['auc'] > best_auc:
        best_auc = val_metrics['auc']
        patience_counter = 0

        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_auc': val_metrics['auc'],
            'val_sensitivity': val_metrics['sensitivity'],
            'history': history
        }, 'best_model.pth')

        print(f"   ‚úÖ Saved best model! (AUC: {val_metrics['auc']:.4f})")
    else:
        patience_counter += 1
        print(f"   ‚è≥ No improvement ({patience_counter}/{EARLY_STOP_PATIENCE})")

    if patience_counter >= EARLY_STOP_PATIENCE:
        print(f"\n‚ö†Ô∏è Early stopping triggered after {epoch + 1} epochs")
        break

print("\n" + "=" * 70)
print(f"üéâ Training Complete!")
print(f"üèÜ Best Validation AUC: {best_auc:.4f}")

def plot_training_history(history):
    """Plot comprehensive training history"""
    fig, axes = plt.subplots(3, 3, figsize=(20, 15))
    axes = axes.reshape(3,3)

    # Loss
    axes[0, 0].plot(history['train_loss'], label='Train', marker='o')
    axes[0, 0].plot(history['val_loss'], label='Val', marker='o')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)

    # AUC
    axes[0, 1].plot(history['val_auc'], marker='o')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('AUC-ROC')
    axes[0, 1].set_title('Validation AUC-ROC')
    axes[0, 1].grid(True)

    # Accuracy
    axes[0, 2].plot(history['val_accuracy'], marker='o')
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('Accuracy')
    axes[0, 2].set_title('Validation Accuracy')
    axes[0, 2].grid(True)

    # F1
    axes[1, 0].plot(history['val_f1'], marker='o')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('F1-Score')
    axes[1, 0].set_title('Validation F1-Score')
    axes[1, 0].grid(True)

    # Precision
    axes[1, 1].plot(history['val_precision'], marker='o')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Precision')
    axes[1, 1].set_title('Validation Precision')
    axes[1, 1].grid(True)

    # Sensitivity (MOST IMPORTANT!)
    axes[1, 2].plot(history['val_sensitivity'], marker='o', linewidth=2)
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('Sensitivity (Recall)')
    axes[1, 2].set_title('Validation SENSITIVITY (Finding Sick!) üéØ', fontweight='bold')
    axes[1, 2].grid(True)

    # Specificity
    axes[2, 0].plot(history['val_specificity'], marker='o')
    axes[2, 0].set_xlabel('Epoch')
    axes[2, 0].set_ylabel('Specificity')
    axes[2, 0].set_title('Validation Specificity')
    axes[2, 0].grid(True)

    # Learning Rate
    axes[2, 1].plot(history['learning_rate'], marker='o')
    axes[2, 1].set_xlabel('Epoch')
    axes[2, 1].set_ylabel('Learning Rate')
    axes[2, 1].set_title('Learning Rate Schedule')
    axes[2, 1].set_yscale('log')
    axes[2, 1].grid(True)

    # Sensitivity vs Specificity
    axes[2, 2].plot(history['val_sensitivity'], label='Sensitivity', marker='o')
    axes[2, 2].plot(history['val_specificity'], label='Specificity', marker='o')
    axes[2, 2].set_xlabel('Epoch')
    axes[2, 2].set_ylabel('Score')
    axes[2, 2].set_title('Sensitivity vs Specificity Trade-off')
    axes[2, 2].legend()
    axes[2, 2].grid(True)

    plt.tight_layout()
    plt.savefig('training_history.png', dpi=150, bbox_inches='tight')
    plt.show()

plot_training_history(history)

print("\nüîç Evaluating on Test Set...")
print("=" * 70)

checkpoint = torch.load('best_model.pth', weights_only=False)
model.load_state_dict(checkpoint['model_state_dict'])

# Find optimal threshold on validation set
val_metrics_for_threshold = validate(model, val_loader, criterion, device, threshold=0.5)
optimal_threshold = find_optimal_threshold(
    val_metrics_for_threshold['labels'],
    val_metrics_for_threshold['predictions']
)
print(f"\nüéØ Optimal Threshold: {optimal_threshold:.3f}")

# Evaluate with optimal threshold
test_metrics = validate(model, test_loader, criterion, device, threshold=optimal_threshold)

print(f"\nüìä TEST SET RESULTS (Threshold={optimal_threshold:.3f}):")
print(f"Loss:        {test_metrics['loss']:.4f}")
print(f"AUC:         {test_metrics['auc']:.4f}")
print(f"Accuracy:    {test_metrics['accuracy']:.4f}")
print(f"F1:          {test_metrics['f1']:.4f}")
print(f"Precision:   {test_metrics['precision']:.4f}")
print(f"SENSITIVITY: {test_metrics['sensitivity']:.4f} üéØ (SICK DETECTION!)")
print(f"Specificity: {test_metrics['specificity']:.4f}")

# Per-disease metrics
print(f"\nüíä PER-DISEASE METRICS:")
print("-" * 90)
print(f"{'Disease':<20} {'AUC':>8} {'Acc':>8} {'F1':>8} {'Prec':>8} {'Sens':>8} {'Spec':>8}")
print("-" * 90)

results = {}
for i, disease in enumerate(DISEASE_LABELS):
    labels = test_metrics['labels'][:, i]
    predictions = test_metrics['predictions'][:, i]
    pred_binary = (predictions > optimal_threshold).astype(int)

    if len(np.unique(labels)) > 1:
        auc_score = roc_auc_score(labels, predictions)
    else:
        auc_score = 0.0

    acc = accuracy_score(labels, pred_binary)
    f1 = f1_score(labels, pred_binary, zero_division=0)
    precision = precision_score(labels, pred_binary, zero_division=0)
    sensitivity = recall_score(labels, pred_binary, zero_division=0)

    # Specificity
    tn = np.sum((labels == 0) & (pred_binary == 0))
    fp = np.sum((labels == 0) & (pred_binary == 1))
    specificity = tn / (tn + fp + 1e-10)

    results[disease] = {
        'AUC': float(auc_score),
        'Accuracy': float(acc),
        'F1': float(f1),
        'Precision': float(precision),
        'Sensitivity': float(sensitivity),
        'Specificity': float(specificity)
    }

    print(f"{disease:<20} {auc_score:>8.3f} {acc:>8.3f} {f1:>8.3f} {precision:>8.3f} {sensitivity:>8.3f} {specificity:>8.3f}")

mean_auc = np.mean([r['AUC'] for r in results.values() if r['AUC'] > 0])
mean_sensitivity = np.mean([r['Sensitivity'] for r in results.values()])

print("-" * 90)
print(f"{'OVERALL MEAN':<20} {mean_auc:>8.3f} {test_metrics['accuracy']:>8.3f} {test_metrics['f1']:>8.3f} {test_metrics['precision']:>8.3f} {mean_sensitivity:>8.3f} {test_metrics['specificity']:>8.3f}")
print("=" * 90)

# Save results
results['_summary'] = {
    'mean_auc': float(mean_auc),
    'mean_sensitivity': float(mean_sensitivity),
    'optimal_threshold': float(optimal_threshold),
    'test_accuracy': float(test_metrics['accuracy']),
    'test_f1': float(test_metrics['f1'])
}

with open('test_results.json', 'w') as f:
    json.dump(results, f, indent=2)

def plot_roc_curves(labels, predictions, disease_names):
    """Plot ROC curves for all diseases"""

    fig, axes = plt.subplots(4, 4, figsize=(20, 20))
    axes = axes.ravel()

    for i, disease in enumerate(disease_names):
        if i < 14 and len(np.unique(labels[:, i])) > 1:
            fpr, tpr, _ = roc_curve(labels[:, i], predictions[:, i])
            roc_auc = auc(fpr, tpr)

            axes[i].plot(fpr, tpr, color='darkorange', lw=2,
                        label=f'ROC curve (AUC = {roc_auc:.3f})')
            axes[i].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
            axes[i].set_xlim([0.0, 1.0])
            axes[i].set_ylim([0.0, 1.05])
            axes[i].set_xlabel('False Positive Rate')
            axes[i].set_ylabel('True Positive Rate')
            axes[i].set_title(f'{disease}')
            axes[i].legend(loc="lower right")
            axes[i].grid(True, alpha=0.3)

    # Hide unused subplots
    for i in range(14, 16):
        axes[i].axis('off')

    plt.tight_layout()
    plt.savefig('roc_curves.png', dpi=150, bbox_inches='tight')
    plt.show()

print("\nüìà Plotting ROC Curves...")
plot_roc_curves(test_metrics['labels'], test_metrics['predictions'], DISEASE_LABELS)

print("\nüì• Downloading Results...")

from google.colab import files

# Download files
files.download('best_model.pth')
files.download('test_results.json')
files.download('training_history.png')
files.download('roc_curves.png')
files.download('disease_distribution.png')
files.download('sample_images.png')

print("\n‚úÖ All Done! Files downloaded.")
print("\nüìä Summary:")
print(f"   - Model: DenseNet-121")
print(f"   - Test AUC: {mean_auc:.4f}")
print(f"   - Test F1: {mean_f1:.4f}")
print(f"   - Best Epoch: {checkpoint['epoch'] + 1}")