# -*- coding: utf-8 -*-
"""Chest X-Ray Disease Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hynyL4dUaeDDqDQZxzcghZ9pdvMLMcZM
"""

# ============================================================================
# PART 1: SETUP & IMPORTS
# ============================================================================

!pip install -q kaggle opendatasets

import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
import json

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torchvision
import torchvision.transforms as transforms
import torchvision.models as models
from PIL import Image

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    roc_auc_score,
    roc_curve,
    auc,
    f1_score,
    precision_score,
    recall_score,
    confusion_matrix
)

# Set random seeds for reproducibility
torch.manual_seed(42)
np.random.seed(42)

print(f"PyTorch Version: {torch.__version__}")
print(f"Torchvision Version: {torchvision.__version__}")
print(f"CUDA Available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")

from google.colab import files

print("üìÅ Please upload your kaggle.json file:")
uploaded = files.upload()

# Setup Kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

print("\nüì• Downloading NIH Chest X-ray Dataset (this may take 5-10 minutes)...")
!kaggle datasets download -d nih-chest-xrays/sample
!unzip -q sample.zip -d chest_xray_data

print("‚úÖ Dataset downloaded!")

# Paths
DATA_DIR = "chest_xray_data/sample/images"
LABELS_FILE = "chest_xray_data/sample_labels.csv"

df = pd.read_csv(LABELS_FILE)

print(f"\nüìä Dataset Overview:")
print(f"Total images: {len(df)}")
print(f"Total patients: {df['Patient ID'].nunique()}")
print(f"\nFirst few rows:")
print(df.head())

# Disease labels
DISEASE_LABELS = [
    'Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax',
    'Edema', 'Emphysema', 'Fibrosis', 'Effusion', 'Pneumonia',
    'Pleural_thickening', 'Cardiomegaly', 'Nodule', 'Mass', 'Hernia'
]

# Create binary columns for each disease
for disease in DISEASE_LABELS:
    df[disease] = df['Finding Labels'].apply(lambda x: 1 if disease in x else 0)

# Count diseases
print(f"\nüíä Disease Distribution:")
disease_counts = df[DISEASE_LABELS].sum().sort_values(ascending=False)
print(disease_counts)

# Visualize disease distribution
plt.figure(figsize=(12, 6))
disease_counts.plot(kind='bar', color='steelblue')
plt.title('Disease Distribution in Dataset', fontsize=14, fontweight='bold')
plt.xlabel('Disease', fontsize=12)
plt.ylabel('Number of Cases', fontsize=12)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.savefig('disease_distribution.png', dpi=100)
plt.show()

# Check for class imbalance
print(f"\n‚öñÔ∏è Class Imbalance Ratio:")
max_count = disease_counts.max()
for disease in disease_counts.index:
    ratio = max_count / disease_counts[disease]
    print(f"{disease:20s}: 1:{ratio:.1f}")

# DATA PREPARATION

class ChestXrayDataset(Dataset):
    """Custom Dataset for Chest X-ray images"""

    def __init__(self, dataframe, image_dir, labels, transform=None):
        self.dataframe = dataframe.reset_index(drop=True)
        self.image_dir = image_dir
        self.labels = labels
        self.transform = transform

    def __len__(self):
        return len(self.dataframe)

    def __getitem__(self, idx):
        # Get image path
        img_name = self.dataframe.iloc[idx]['Image Index']
        img_path = os.path.join(self.image_dir, img_name)

        # Load image
        try:
            image = Image.open(img_path).convert('RGB')
        except:
            # If image fails to load, return a black image
            image = Image.new('RGB', (224, 224), color='black')
            print(f"Warning: Failed to load {img_name}")

        # Get labels
        label_vector = self.dataframe.iloc[idx][self.labels].values.astype(np.float32)

        # Apply transforms
        if self.transform:
            image = self.transform(image)

        return image, torch.tensor(label_vector)

# Data transforms with augmentation
train_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.RandomHorizontalFlip(p=0.5),
    transforms.RandomRotation(10),
    transforms.ColorJitter(brightness=0.2, contrast=0.2),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

val_transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                        std=[0.229, 0.224, 0.225])
])

def split_dataset(df, test_size=0.15, val_size=0.15, random_state=42):
    """Split dataset by patient ID to prevent data leakage"""

    patients = df['Patient ID'].unique()

    # Split patients
    train_patients, test_patients = train_test_split(
        patients, test_size=test_size, random_state=random_state
    )
    train_patients, val_patients = train_test_split(
        train_patients, test_size=val_size/(1-test_size), random_state=random_state
    )

    # Create dataframes
    train_df = df[df['Patient ID'].isin(train_patients)]
    val_df = df[df['Patient ID'].isin(val_patients)]
    test_df = df[df['Patient ID'].isin(test_patients)]

    print(f"\nüìä Data Split:")
    print(f"Train: {len(train_df)} images ({len(train_patients)} patients)")
    print(f"Val:   {len(val_df)} images ({len(val_patients)} patients)")
    print(f"Test:  {len(test_df)} images ({len(test_patients)} patients)")

    return train_df, val_df, test_df

# Split data
train_df, val_df, test_df = split_dataset(df)

# Create datasets
train_dataset = ChestXrayDataset(train_df, DATA_DIR, DISEASE_LABELS, train_transform)
val_dataset = ChestXrayDataset(val_df, DATA_DIR, DISEASE_LABELS, val_transform)
test_dataset = ChestXrayDataset(test_df, DATA_DIR, DISEASE_LABELS, val_transform)

# Create dataloaders
BATCH_SIZE = 32

train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True,
                         num_workers=2, pin_memory=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False,
                       num_workers=2, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False,
                        num_workers=2, pin_memory=True)

print(f"\n‚úÖ Dataloaders created:")
print(f"Train batches: {len(train_loader)}")
print(f"Val batches: {len(val_loader)}")
print(f"Test batches: {len(test_loader)}")

# Visualize sample images
def show_sample_images(dataloader, num_images=8):
    """Display sample images from dataloader"""

    images, labels = next(iter(dataloader))

    fig, axes = plt.subplots(2, 4, figsize=(16, 8))
    axes = axes.ravel()

    for i in range(min(num_images, len(images))):
        img = images[i].permute(1, 2, 0).numpy()

        # Denormalize
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        img = std * img + mean
        img = np.clip(img, 0, 1)

        axes[i].imshow(img)

        # Get disease names
        label_indices = torch.where(labels[i] == 1)[0]
        if len(label_indices) > 0:
            diseases = [DISEASE_LABELS[idx] for idx in label_indices]
            title = ', '.join(diseases[:3])  # Show max 3 diseases
            if len(diseases) > 3:
                title += f' +{len(diseases)-3} more'
        else:
            title = 'No Finding'

        axes[i].set_title(title, fontsize=10)
        axes[i].axis('off')

    plt.tight_layout()
    plt.savefig('sample_images.png', dpi=100)
    plt.show()

print("\nüñºÔ∏è Sample Images:")
show_sample_images(train_loader)

class ChestXrayModel(nn.Module):
    """DenseNet-121 for multi-label chest X-ray classification"""

    def __init__(self, num_classes=14, pretrained=True, dropout=0.5):
        super(ChestXrayModel, self).__init__()

        # Load pre-trained DenseNet-121
        self.densenet = models.densenet121(pretrained=pretrained)

        # Get number of features
        num_features = self.densenet.classifier.in_features

        # Replace classifier
        self.densenet.classifier = nn.Sequential(
            nn.Dropout(p=dropout),
            nn.Linear(num_features, num_classes)
        )

    def forward(self, x):
        return self.densenet(x)

# Create model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = ChestXrayModel(num_classes=14, pretrained=True, dropout=0.5)
model = model.to(device)

# Count parameters
total_params = sum(p.numel() for p in model.parameters())
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

print(f"\nüß† Model Architecture:")
print(f"Total parameters: {total_params:,}")
print(f"Trainable parameters: {trainable_params:,}")
print(f"Device: {device}")

# Calculate class weights for imbalanced dataset
def calculate_class_weights(train_df, labels):
    """Calculate pos_weight for BCEWithLogitsLoss"""

    pos_counts = train_df[labels].sum().values
    neg_counts = len(train_df) - pos_counts

    # pos_weight = neg_count / pos_count
    pos_weights = neg_counts / (pos_counts + 1)  # +1 to avoid division by zero

    print(f"\n‚öñÔ∏è Class Weights (pos_weight):")
    for label, weight in zip(labels, pos_weights):
        print(f"{label:20s}: {weight:.2f}")

    return torch.tensor(pos_weights, dtype=torch.float32)

# Get class weights
class_weights = calculate_class_weights(train_df, DISEASE_LABELS).to(device)

# Loss function with class weights
criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights)

# Optimizer
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# Learning rate scheduler
scheduler = optim.lr_scheduler.ReduceLROnPlateau(
    optimizer,
    mode='max',      # Maximize AUC
    factor=0.5,      # Reduce LR by half
    patience=2,      # Wait 2 epochs
    min_lr=1e-7
)

print(f"\n‚úÖ Training Setup:")
print(f"Loss: BCEWithLogitsLoss (weighted)")
print(f"Optimizer: Adam (lr=0.0001)")
print(f"Scheduler: ReduceLROnPlateau")

def train_one_epoch(model, loader, criterion, optimizer, device):
    """Train for one epoch"""
    model.train()
    running_loss = 0.0

    progress_bar = tqdm(loader, desc="Training")

    for images, labels in progress_bar:
        images = images.to(device)
        labels = labels.to(device)

        # Forward pass
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass
        loss.backward()
        optimizer.step()

        running_loss += loss.item()
        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

    return running_loss / len(loader)

def validate(model, loader, criterion, device, threshold=0.5):
    """Validate model and calculate metrics"""
    model.eval()
    running_loss = 0.0
    all_labels = []
    all_predictions = []

    with torch.no_grad():
        for images, labels in tqdm(loader, desc="Validating"):
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            running_loss += loss.item()

            # Get predictions (sigmoid for probabilities)
            predictions = torch.sigmoid(outputs).cpu().numpy()
            all_predictions.append(predictions)
            all_labels.append(labels.cpu().numpy())

    # Concatenate all batches
    all_predictions = np.vstack(all_predictions)
    all_labels = np.vstack(all_labels)

    # Calculate metrics
    avg_loss = running_loss / len(loader)

    # AUC-ROC per disease
    auc_scores = []
    for i in range(len(DISEASE_LABELS)):
        if len(np.unique(all_labels[:, i])) > 1:
            auc = roc_auc_score(all_labels[:, i], all_predictions[:, i])
            auc_scores.append(auc)
        else:
            auc_scores.append(0.0)

    mean_auc = np.mean([s for s in auc_scores if s > 0])

    # Binary predictions
    predictions_binary = (all_predictions > threshold).astype(int)

    # Accuracy (exact match for multi-label)
    exact_match_accuracy = np.mean(np.all(predictions_binary == all_labels, axis=1))

    # Hamming accuracy (per-label accuracy)
    hamming_accuracy = np.mean(predictions_binary == all_labels)

    # F1, Precision, Recall
    f1 = f1_score(all_labels, predictions_binary, average='macro', zero_division=0)
    precision = precision_score(all_labels, predictions_binary, average='macro', zero_division=0)
    recall = recall_score(all_labels, predictions_binary, average='macro', zero_division=0)

    return {
        'loss': avg_loss,
        'auc': mean_auc,
        'f1': f1,
        'precision': precision,
        'recall': recall,
        'exact_match_accuracy': exact_match_accuracy,
        'hamming_accuracy': hamming_accuracy,
        'auc_per_disease': auc_scores,
        'predictions': all_predictions,
        'labels': all_labels
    }

NUM_EPOCHS = 15
EARLY_STOP_PATIENCE = 5

print(f"\nüöÄ Starting Training for {NUM_EPOCHS} epochs")
print("=" * 70)

# Training history
history = {
    'train_loss': [],
    'val_loss': [],
    'val_auc': [],
    'val_f1': [],
    'val_precision': [],
    'val_recall': [],
    'learning_rate': []
}

best_auc = 0.0
patience_counter = 0

for epoch in range(NUM_EPOCHS):
    print(f"\nüìç Epoch {epoch + 1}/{NUM_EPOCHS}")
    print("-" * 70)

    # Train
    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)

    # Validate
    val_metrics = validate(model, val_loader, criterion, device)

    # Update scheduler
    scheduler.step(val_metrics['auc'])
    current_lr = optimizer.param_groups[0]['lr']

    # Save history
    history['train_loss'].append(train_loss)
    history['val_loss'].append(val_metrics['loss'])
    history['val_auc'].append(val_metrics['auc'])
    history['val_f1'].append(val_metrics['f1'])
    history['val_precision'].append(val_metrics['precision'])
    history['val_recall'].append(val_metrics['recall'])
    history['learning_rate'].append(current_lr)

    # Print results
    print(f"\nüìä Results:")
    print(f"   Train Loss:  {train_loss:.4f}")
    print(f"   Val Loss:    {val_metrics['loss']:.4f}")
    print(f"   Val AUC:     {val_metrics['auc']:.4f} ‚≠ê")
    print(f"   Val F1:      {val_metrics['f1']:.4f}")
    print(f"   Val Prec:    {val_metrics['precision']:.4f}")
    print(f"   Val Recall:  {val_metrics['recall']:.4f}")
    print(f"   Learning Rate: {current_lr:.2e}")

    # Save best model
    if val_metrics['auc'] > best_auc:
        best_auc = val_metrics['auc']
        patience_counter = 0

        torch.save({
            'epoch': epoch,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': optimizer.state_dict(),
            'val_auc': val_metrics['auc'],
            'val_f1': val_metrics['f1'],
            'history': history
        }, 'best_model.pth')

        print(f"   ‚úÖ Saved best model! (AUC: {val_metrics['auc']:.4f})")
    else:
        patience_counter += 1
        print(f"   ‚è≥ No improvement ({patience_counter}/{EARLY_STOP_PATIENCE})")

    # Early stopping
    if patience_counter >= EARLY_STOP_PATIENCE:
        print(f"\n‚ö†Ô∏è Early stopping triggered after {epoch + 1} epochs")
        break

print("\n" + "=" * 70)
print(f"üéâ Training Complete!")
print(f"üèÜ Best Validation AUC: {best_auc:.4f}")

def plot_training_history(history):
    """Plot training history"""

    fig, axes = plt.subplots(2, 3, figsize=(18, 10))

    # Loss
    axes[0, 0].plot(history['train_loss'], label='Train Loss', marker='o')
    axes[0, 0].plot(history['val_loss'], label='Val Loss', marker='o')
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('Loss')
    axes[0, 0].set_title('Training & Validation Loss')
    axes[0, 0].legend()
    axes[0, 0].grid(True)

    # AUC
    axes[0, 1].plot(history['val_auc'], marker='o', color='green')
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('AUC-ROC')
    axes[0, 1].set_title('Validation AUC-ROC')
    axes[0, 1].grid(True)

    # F1
    axes[0, 2].plot(history['val_f1'], marker='o', color='orange')
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('F1-Score')
    axes[0, 2].set_title('Validation F1-Score')
    axes[0, 2].grid(True)

    # Precision
    axes[1, 0].plot(history['val_precision'], marker='o', color='blue')
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('Precision')
    axes[1, 0].set_title('Validation Precision')
    axes[1, 0].grid(True)

    # Recall
    axes[1, 1].plot(history['val_recall'], marker='o', color='red')
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('Recall')
    axes[1, 1].set_title('Validation Recall')
    axes[1, 1].grid(True)

    # Learning Rate
    axes[1, 2].plot(history['learning_rate'], marker='o', color='purple')
    axes[1, 2].set_xlabel('Epoch')
    axes[1, 2].set_ylabel('Learning Rate')
    axes[1, 2].set_title('Learning Rate Schedule')
    axes[1, 2].set_yscale('log')
    axes[1, 2].grid(True)

    plt.tight_layout()
    plt.savefig('training_history.png', dpi=150, bbox_inches='tight')
    plt.show()

plot_training_history(history)

print("\nüîç Evaluating on Test Set...")
print("=" * 70)

# Load best model
checkpoint = torch.load('best_model.pth', weights_only=False)
model.load_state_dict(checkpoint['model_state_dict'])

# Evaluate
test_metrics = validate(model, test_loader, criterion, device, threshold=0.5)

print(f"\nüìä TEST SET RESULTS:")
print(f"Loss:      {test_metrics['loss']:.4f}")
print(f"AUC:       {test_metrics['auc']:.4f}")
print(f"F1:        {test_metrics['f1']:.4f}")
print(f"Precision: {test_metrics['precision']:.4f}")
print(f"Recall:    {test_metrics['recall']:.4f}")

# Per-disease metrics
print(f"\nüíä PER-DISEASE METRICS:")
print("-" * 70)
print(f"{'Disease':<20} {'AUC':>8} {'F1':>8} {'Precision':>10} {'Recall':>10}")
print("-" * 70)

results = {}
for i, disease in enumerate(DISEASE_LABELS):
    labels = test_metrics['labels'][:, i]
    predictions = test_metrics['predictions'][:, i]
    pred_binary = (predictions > 0.5).astype(int)

    if len(np.unique(labels)) > 1:
        auc_score = roc_auc_score(labels, predictions)
    else:
        auc_score = 0.0

    f1 = f1_score(labels, pred_binary, zero_division=0)
    precision = precision_score(labels, pred_binary, zero_division=0)
    recall = recall_score(labels, pred_binary, zero_division=0)

    results[disease] = {
        'AUC': float(auc_score),
        'F1': float(f1),
        'Precision': float(precision),
        'Recall': float(recall)
    }

    print(f"{disease:<20} {auc_score:>8.3f} {f1:>8.3f} {precision:>10.3f} {recall:>10.3f}")

# Overall
mean_auc = np.mean([r['AUC'] for r in results.values() if r['AUC'] > 0])
mean_f1 = np.mean([r['F1'] for r in results.values()])

print("-" * 70)
print(f"{'OVERALL MEAN':<20} {mean_auc:>8.3f} {mean_f1:>8.3f}")
print("=" * 70)

# Save results
with open('test_results.json', 'w') as f:
    json.dump(results, f, indent=2)

def plot_roc_curves(labels, predictions, disease_names):
    """Plot ROC curves for all diseases"""

    fig, axes = plt.subplots(4, 4, figsize=(20, 20))
    axes = axes.ravel()

    for i, disease in enumerate(disease_names):
        if i < 14 and len(np.unique(labels[:, i])) > 1:
            fpr, tpr, _ = roc_curve(labels[:, i], predictions[:, i])
            roc_auc = auc(fpr, tpr)

            axes[i].plot(fpr, tpr, color='darkorange', lw=2,
                        label=f'ROC curve (AUC = {roc_auc:.3f})')
            axes[i].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
            axes[i].set_xlim([0.0, 1.0])
            axes[i].set_ylim([0.0, 1.05])
            axes[i].set_xlabel('False Positive Rate')
            axes[i].set_ylabel('True Positive Rate')
            axes[i].set_title(f'{disease}')
            axes[i].legend(loc="lower right")
            axes[i].grid(True, alpha=0.3)

    # Hide unused subplots
    for i in range(14, 16):
        axes[i].axis('off')

    plt.tight_layout()
    plt.savefig('roc_curves.png', dpi=150, bbox_inches='tight')
    plt.show()

print("\nüìà Plotting ROC Curves...")
plot_roc_curves(test_metrics['labels'], test_metrics['predictions'], DISEASE_LABELS)

print("\nüì• Downloading Results...")

from google.colab import files

# Download files
files.download('best_model.pth')
files.download('test_results.json')
files.download('training_history.png')
files.download('roc_curves.png')
files.download('disease_distribution.png')
files.download('sample_images.png')

print("\n‚úÖ All Done! Files downloaded.")
print("\nüìä Summary:")
print(f"   - Model: DenseNet-121")
print(f"   - Test AUC: {mean_auc:.4f}")
print(f"   - Test F1: {mean_f1:.4f}")
print(f"   - Best Epoch: {checkpoint['epoch'] + 1}")

